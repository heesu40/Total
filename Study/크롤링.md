## python으로 크롤링

### 1. 주로 사용하는 라이브러리

- Requests : 파이썬에서 동작하는 작고 빠른 브라우저
  - 웹서버로부터 초기 HTML만 받을 뿐, 추가 CSS/JavsScript처리 없다.
  - 거의 모든 플랫폼에서 구동가능
- Selenium : 브라우저가 아니고 브라우저를 원격 컨트롤하는 테스팅 라이브러리
  - Chrome, Firefox, IE, PhantomJS등
  - 기본 브라우저를 사용하므로, 추가 CSS/JavaScript처리 지원 가능
  - 리소스를 많이 먹는다. 사이트에 따라 죽기도 한다.
- BeautifulSoup4: HTML 파서
  - 지정 HTML로부터 원하는 위치/형식의 문자열을 획득
  - 주로 Requests에 의해 많이 사용되지만, Selenium에서도 사용할 수 있다.

### 2. Selenium이 좋은 경우

- 고민하기 싫다.
- 내가 요청할 페이지가 몇 개 안된다.
- GUI가 있는 컴퓨터에서 수행한다.
- 컴퓨터 사양이 넉넉하다.
- 웹페이지가 JavaScript로 동작한다.
- => 위의 경우 중 하나라도 포함된다면 Selenium이 좋다.

### 3. 내가 원하는 컨텐츠가 별도 Ajax요청을 통해 받아오진 않는지?

- 이는 브라우저 개발자 도구의 Network탭을 통해 확인 가능하다.
- 받아오고 있다면 Requests로!

### 4. 원하는 컨텐츠가 별도 자바스크립트 로직을 통해 그려지는가?

- 자바스크립트 로직을 통해 그려진다면 로직으로 파이썬 로직으로 변환 가능한지 체크!
  - 변환 가능 => 파이썬 로직으로 변경해서 처리
  - 변환 불가 => Selenium 사용
- 모르겠거나 그렇지 않으면 Selenium사용

### 5. 페이지 소스 메뉴로 봤을 때, 내가 원하는 컨텐츠가 있는가?

- 있으면 Requests로

### 6. ActiveX사이트

- Selenium으로 IE를 통해 자동화 시도
- 하지만 매번 ActiveX설치 이벤트 처리 까다로우므로 가급적 피해가기

### 7. Requests 크롤링 tip!

- 요청 시에 종종 추가 헤더  설정 필요할 수 있다. 헤더를 설정하지 않으면 응답을 안해주는 서비스가 있다.
  - Referer
  - User-Agent
  - Accept-Language
- 모바일 페이지가 있다면 모바일 페이지로 크롤링. HTML마크업보다 심플하다.
- 크롤링시 파이썬파일로 인식할 때가 있는데 이를 크롬이나 다른 브라우저로 인식하기 위해서 [참고사이트,user agent string](http://www.useragentstring.com/)를 참고한다.
- ![스크린샷 2020-03-17 오전 11.46.20](크롤링.assets/스크린샷 2020-03-17 오전 11.46.20.png)
- 



###  총체적 정리

1. 단순 HTML 크롤링
   - Requests
2. Ajax 렌더링 크롤링
   - Requests로 해볼 수 있으면 하고 어려우면 Selenium
3. AngularJS, Vue.js, React.JS 류의 자바스크립트 렌더링 크롤링
   - Selenium으로 
   - 분석하기에 따라 requests로 가능
4. 이도저도 아니며 생각하기 힘들면 Selenium

### 사이트별 라이브 데모

- 네이버 실시간 검색어 : 정적 HTML
- 멜론 검색: Ajax 렌더링
- .....추가



## 현 크롬 페이지에서 크롤링

참고 [주소](https://klonic.tistory.com/76)

- 먼저 chrome.exe파일 위치를 찾는다.
- 그곳에서 주소창에 `cmd.`를 누르면 폴더위치를 가진 cmd창이 열린다.
- `chrome.exe --remote-debugging-port-9222 --user-data-dir="C:ChromeTEMP"`  경로이름은 아무거나 상관없으며 나중에 삭제하자 여러가지 파일로 나오므로 정리해야한다.
- 실행과 동시에 열리는 chrome창을 확인하고 버그용 크롬창이며 이것이 실제 열렸는지 확인을 위해서는 다른 크롬창을 열고 `127.0.0.1:9222`를 주소창에 넣어 엔터를 치면 확인가능하다
- 버그확인창이 곧 현 페이지 내에서 크롤링할 페이지이다.

### 크롤링

```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import requests
from bs4 import BeautifulSoup
import urllib
import time

chrome_options = Options()
chrome_options.add_experimental_option("debuggerAddress", "127.0.0.1:9222")
chrome_driver = "C:/Users/user/Downloads/chromedriver.exe" #chromedriver.exe가 필요하며 그 위치를 넣어준다.
driver = webdriver.Chrome(chrome_driver, options = chrome_options)

for i in range(21, 22):
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser" )
    datatext = soup.select('div.noveltext')
    print(str(i) + "프린트시작")
    name = str(i) + ".txt"
    f = open(name, "w", encoding="utf-16" )
    for ll in datatext:
        f.write(ll.text)
        
    f.close()
#현 페이지 내에서 잘 뽑힘을 확인 가능하다.
```



