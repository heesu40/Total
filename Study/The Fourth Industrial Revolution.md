# The Fourth Industrial Revolution 

1. 수학공부
   - 객관성부여시
2. 프로그래밍공부
   - 남이 만든 코드 수정할 정도
3. 책 읽어보자
   - 특이점이 온다
   - 사차산업혁명(클라우스 슈밥?)

전시간 복습(entropy를 구해야 디션트리를 구할 수 있는데 )

1. 확률을 구할 수 있어야 하며
2. 확률을 구하기 위해서는 셀수 있어야 한다
3. 그렇다고 모두 숫자일 필요는 없다.( 예를 들어 김씨 사원의 명수 '김' 은 문자다)



## Deep Learning?

- 을 배우려면 Artificial Neural Network (인공신경망)을 알아야하고(+미분을 알아야 한다.)
- 인공신경망을 알기 위해서 perceptron알아야 하며
- perceptron알기 위해서는 Linear clessifier를 알아야 한다.

## Parameter Learning

1. 모델에 사용될 속성
   - ***Domain Knowledge 이용***
     - 도메인 전문가를 불러서 정보를 얻거나 실험을 통해 최적의 정보를 찾아야 한다.
   - ***Data Mining 기법 이용***
     - Parameter Learning or Parametric Modiling
       - 수식안의 결과를 잘 보여주는 상수 찾기
     - Linear 모델 기반
2. 가정 
   - 분류와 계층 확률 추정 시 2진 계층만 고려(글자가 들어오면 안된다. 수치형으로 바꾸자)
   - 모든 속성이 수치형이라 가정
   - 수치형 데이터의 공통단위 표준화 필요 없음(알고리즘이 목적이기에 되어있다 가정한다.)
3. 목표
   - 최적의 모델 파라미터를 찾아 모델을 데이터에 맞추기
   - 약간의 수학 필요

### 함수이용분류(Classification via Mathematical Functions)

- 트리모델을 객체 공간 그래프로 변환
- n차원의 공간의 분류는 n-1차원이다.
  - Occam's razor- 오캄의 면도날(simple is best)
- 2차 함수의 경우 공간을 분류할때 1차 함수가 되어 직선이된다.
- **weight**-가중치
- 여기서는 일차방정식이므로 **기울기** 와**절편**을 구하면 된다.(중요한 상수)



#### 찾아가는 과정(이진분류) 첫번째

1. 킬로그램 =파운트 * x 라는 ***선형관계*** 존재
2. 실제 데이터 파운드 100 일때 킬로그램 45.359237(Supervised Learning)
3. x 에 임의의 값 0.3 입력
   - 100*0.3=30       오차=15.359237(* *에러* 틀린것이 아니다)
4. 0.5 입력 
   - 오차는 -4.640763 정답을 넘어섬..
5. 이를 줄이면서 찾는다.

#### 찾아가는 과정(이진분류)  두번째

| 무게 | 길이 | 어종 |
| ---- | ---- | ---- |
| 25   | 20   | 연어 |
| 10   | 50   | 농어 |

1. 무게 25, 길이 20인 연어(임의의 기울기 0.66)
2. y=0.66*25 =16.5 (우리가 만든 모델 예측값)
3. 오차= 목표값-출력값  8.5=25-16.5
4. 선형분류 함수 y=ax  
5. 목표값 t=(a+△a)x
6. E= t-y =(a+△a)x-ax =△ax
7. △a=E//x
8. 위의 식을 이용하여 무게 25, 길이 20인 연어 데이터는
9. 8.5/25 =0.34 가 되어 최종 기울기는 t=(a+△a)x=(0.66+0.34)x=1.0x
10. 무게 10이고 길이 50인 농어의 경우
11. 오차=49-10(목표값-출력값) 임으로 △a=E/x =39/10=3.9
12. y=(a+△a)x 이므로 y=(1+3.9)x 이 되며 y=4.9x

#### 기울기 업데이트의 문제점

1. 마지막 데이터의 영향력이 너무 강해 보완하는 방법으로 학습률 이용.   η을 0.5라 가정하면 좀더 부드럽게 분류가 가능하다.

`t=η(a+△a)x `

2. XOR문제 의 경우 분류를 할 수 없다.

​     

### 선형판별 함수

`f(x) = w0 +w1x1+w2x2+......`

이 식의 뜻은 모든 데이터(x)는 각각, 즉 데이터 하나마다 구별하는 선을 가지고 있으므로 이 선을 모두 더해 하나의 선을 구하는 것! 이라는 의미의 함수이다.



#### 목적함수 최적화

1. 목표 설정 후 목적함수 정의
2. 특정 데이터 세트의 대한 가중치 집합 계산
3. 최대, 최소 가중치는 지역최소값, 지역 최대값이라 표현하자
4. DM에서 최적의 목적함수 찾기는 불가능 하다
   - 로지스틱 회귀분석(Logistic Regrssion)
   - SVM(Support Vector Machine)- 다양하게 분류 될 수 있는 여러가지 선을 두께를 늘려 옴짤달짝 못하는 정도를 구해 두꺼워진 선 (평행한 두 선)의 중 선을 SVM이라 한다. 차원을 n차원까지 늘려..........
5. f(x)=ax+b 에서 f(x)값이 0이면 선위에 있기 때문에 분류 할수 없다. 그렇다면 f(x)값이 0과 가깝다면 선과 가까울 것이고 f(x)가 크면 그렇지 않을 것이다. 그렇다면 분류값이 쉽게 안바뀌는 것은  f(x)가 0에 가까울때일까??? 아닐까?
   - 정답은 f(x)가 0과 멀수록! 왜냐하면 0과 가까울수록 기울기가 조금만 바뀌어도 쉽게 분류장소가 바뀐다. 



#### 회귀분석(예측은 예측인데 특정한 숫자를 예측)

- **얼마나 많이**라는 말을 붙였을때 말이 되면 그것은 회귀 분석이다.

- 선형회귀분석모델

  `f(x) = w0 +w1x1+w2x2+......` (선형판별함수와 식이 같다)

- ^y(종속변수, 내가 알고 싶은 값, 추정치임으로 위에 뚜껑을 씌워준다.)=a+bx(x는 독립변수, 수요에 가장 큰 영향을 미치는 요인, b는 직선의 기울기)

> 선형회귀분석모델(다양한 값의 중점을 지나는 직선, 추세보는 것)
>
> 선형판별함수(선을 이용해 값을 분류 ) 
>
> 식은 같지만 위아래의 의미는 다르다. 그렇기에 잘못하면 판별할때 이상해 질수 있다. 

**다변량회귀분석**

### 계층확률 추정과 로지스틱 회귀분석

- 승산(Odds)
  - 사건이 일어날 가능성을 표현하는 방법 중 하나
  - 0.5 확률 50:50 즉 1
  - 0.9 확률 90:10 즉 9
  - `odds=e^(w0+w1x1+w2x2+...)` 라 표현한다.
  - `ln(Odds)=w0+w1x1+w2x2+....`

## Logistic Regression

### Supervised Learning

### 목적

1. A와 B 두 카테고리 중 하나로 분류

2. 선형 회귀분석 식에서 

   - 우리가 예측하려는Y 값을 A일 확률로 가정
   - Y값(A일 확률)이 0.5보다 크면 A로 분류하고 0.5보다 작으면 B라고 분류

3. 수식전개

   1. 선형회귀 식 y=ax+b

   2. y를 확률로 변환 P=ax+b 이때 양변이 갖는 값의 범위가 일치하지 않음

      

## Single Layer Perceptron

- 신경망의 단군할아버지??
- Perceptron이 붙어있으면 무조건 이진분류밖에 못한다.
- step function를 사용한다.(계단식 그래프) 하지만 신경망을 들어가면 step function 을 사용하지 못한다 . 

![ê´ë ¨ ì´ë¯¸ì§](C:\Users\student\Documents\STUDY\javaStudy\사진\step function)

- Sigmoid function 을 사용한다.(왜?  step funcition 은 연속이 아니라 미분이 안된다. 0값에서 y 값이 2개이므로) step function 을 양옆으로 살짝 늘린것으로써 미분 가능!

![sigmoid functionì ëí ì´ë¯¸ì§ ê²ìê²°ê³¼](C:\Users\student\Documents\STUDY\javaStudy\사진\sigmoid function)

- Relu function

![Relu functionì ëí ì´ë¯¸ì§ ê²ìê²°ê³¼](C:\Users\student\Documents\STUDY\javaStudy\사진\Relu function)

